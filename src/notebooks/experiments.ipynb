{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7b4f08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import lightning as L\n",
    "import torchmetrics \n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "import itertools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def load_wildppg_participant(path):\n",
    "    \"\"\"\n",
    "    Loads the data of a WildPPG participant and cleans it to receive nested dictionaries\n",
    "    \"\"\"\n",
    "    loaded_data = scipy.io.loadmat(path)\n",
    "    loaded_data['id'] = loaded_data['id'][0]\n",
    "    if len(loaded_data['notes'])==0:\n",
    "        loaded_data['notes']=\"\"\n",
    "    else:\n",
    "        loaded_data['notes']=loaded_data['notes'][0]\n",
    "\n",
    "    for bodyloc in ['sternum', 'head', 'wrist', 'ankle']:\n",
    "        bodyloc_data = dict() # data structure to feed cleaned data into\n",
    "        sensors = loaded_data[bodyloc][0].dtype.names\n",
    "        for sensor_name, sensor_data in zip(sensors, loaded_data[bodyloc][0][0]):\n",
    "            bodyloc_data[sensor_name] = dict()\n",
    "            field_names = sensor_data[0][0].dtype.names\n",
    "            for sensor_field, field_data in zip(field_names, sensor_data[0][0]):\n",
    "                bodyloc_data[sensor_name][sensor_field] = field_data[0]\n",
    "                if sensor_field == 'fs':\n",
    "                    bodyloc_data[sensor_name][sensor_field] = bodyloc_data[sensor_name][sensor_field][0]\n",
    "        loaded_data[bodyloc] = bodyloc_data\n",
    "    return loaded_data\n",
    "\n",
    "def panPeakDetect(detection, fs):\n",
    "    \"\"\"\n",
    "    Jiapu Pan and Willis J. Tompkins.\n",
    "    A Real-Time QRS Detection Algorithm.\n",
    "    In: IEEE Transactions on Biomedical Engineering\n",
    "    BME-32.3 (1985), pp. 230–236.\n",
    "\n",
    "    Original implementation by Luis Howell luisbhowell@gmail.com, Bernd Porr, bernd.porr@glasgow.ac.uk, DOI: 10.5281/zenodo.3353396\n",
    "    \"\"\"\n",
    "    min_distance = int(0.25 * fs)\n",
    "\n",
    "    signal_peaks = [0]\n",
    "    noise_peaks = []\n",
    "\n",
    "    SPKI = 0.0\n",
    "    NPKI = 0.0\n",
    "\n",
    "    threshold_I1 = 0.0\n",
    "    threshold_I2 = 0.0\n",
    "\n",
    "    RR_missed = 0\n",
    "    indexes = []\n",
    "\n",
    "    missed_peaks = []\n",
    "    peaks = scipy.signal.find_peaks(detection,distance=min_distance)[0]\n",
    "\n",
    "    thres_weight = 0.125\n",
    "\n",
    "    for index, peak in enumerate(peaks):\n",
    "\n",
    "        if peak>4*fs and threshold_I1>max(detection[peak-4*fs:peak]): # reset thresholds if we do not see any peaks anymore\n",
    "            SPKI_n = max(detection[peak-4*fs:peak])\n",
    "            NPKI = min(NPKI*SPKI_n/SPKI, np.percentile(detection[peak-4*fs:peak], 80))\n",
    "            SPKI = SPKI_n\n",
    "            threshold_I1 = NPKI + 0.25 * (SPKI - NPKI)\n",
    "            threshold_I2 = 0.5 * threshold_I1\n",
    "\n",
    "\n",
    "\n",
    "        if detection[peak] > threshold_I1 and (peak - signal_peaks[-1]) > 0.3 * fs:\n",
    "            signal_peaks.append(peak)\n",
    "            indexes.append(index)\n",
    "            SPKI = thres_weight * detection[signal_peaks[-1]] + (1-thres_weight) * SPKI\n",
    "            if RR_missed != 0:\n",
    "                if signal_peaks[-1] - signal_peaks[-2] > RR_missed:\n",
    "                    missed_section_peaks = peaks[indexes[-2] + 1:indexes[-1]]\n",
    "                    missed_section_peaks2 = []\n",
    "                    for missed_peak in missed_section_peaks:\n",
    "                        if missed_peak - signal_peaks[-2] > min_distance and signal_peaks[\n",
    "                            -1] - missed_peak > min_distance and detection[missed_peak] > threshold_I2:\n",
    "                            missed_section_peaks2.append(missed_peak)\n",
    "\n",
    "                    if len(missed_section_peaks2) > 0:\n",
    "                        signal_missed = [detection[i] for i in missed_section_peaks2]\n",
    "                        index_max = np.argmax(signal_missed)\n",
    "                        missed_peak = missed_section_peaks2[index_max]\n",
    "                        missed_peaks.append(missed_peak)\n",
    "                        signal_peaks.append(signal_peaks[-1])\n",
    "                        signal_peaks[-2] = missed_peak\n",
    "            if len(signal_peaks)>100 and thres_weight>0.1:\n",
    "                thres_weight = 0.0125\n",
    "\n",
    "\n",
    "        else:\n",
    "            noise_peaks.append(peak)\n",
    "            NPKI = thres_weight * detection[noise_peaks[-1]] + (1-thres_weight) * NPKI\n",
    "\n",
    "        threshold_I1 = NPKI + 0.25 * (SPKI - NPKI)\n",
    "        threshold_I2 = 0.5 * threshold_I1\n",
    "\n",
    "        if len(signal_peaks) > 8:\n",
    "            RR = np.diff(signal_peaks[-9:])\n",
    "            RR_ave = int(np.mean(RR))\n",
    "            RR_missed = int(1.66 * RR_ave)\n",
    "\n",
    "    signal_peaks.pop(0)\n",
    "\n",
    "    return signal_peaks\n",
    "\n",
    "\n",
    "def pan_tompkins_detector(unfiltered_ecg, sr):\n",
    "    \"\"\"\n",
    "    Jiapu Pan and Willis J. Tompkins.\n",
    "    A Real-Time QRS Detection Algorithm.\n",
    "    In: IEEE Transactions on Biomedical Engineering\n",
    "    BME-32.3 (1985), pp. 230–236.\n",
    "\n",
    "    Original implementation by Luis Howell luisbhowell@gmail.com, Bernd Porr, bernd.porr@glasgow.ac.uk, DOI: 10.5281/zenodo.3353396\n",
    "    \"\"\"\n",
    "    maxQRSduration = 0.150  # sec\n",
    "    filtered_ecg = butter_bandpass_filter(unfiltered_ecg, 5, 15, sr, order=1)\n",
    "\n",
    "    diff = np.diff(filtered_ecg)\n",
    "    squared = diff * diff\n",
    "\n",
    "    mwa = scipy.ndimage.uniform_filter1d(squared, size=int(maxQRSduration * sr))\n",
    "    # cap mwa during motion artefacts to make sure it does not screw the thresholds\n",
    "    maxvals = scipy.ndimage.maximum_filter1d(filtered_ecg, size=int(maxQRSduration * sr))[:-1]/400\n",
    "    mwa = np.asarray([v if v < maxval else maxval for maxval, v in zip(maxvals, mwa)])\n",
    "\n",
    "    mwa[:int(maxQRSduration * sr * 2)] = 0\n",
    "\n",
    "    searchr = int(maxQRSduration * sr)\n",
    "    peakfind = butter_bandpass_filter(unfiltered_ecg, 7.5, 20, sr, order=1)\n",
    "\n",
    "    mwa_peaks = panPeakDetect(mwa, sr)\n",
    "    r_peaks2 = []\n",
    "    for rp in mwa_peaks:\n",
    "        r_peaks2.append(rp - searchr + np.argmax(peakfind[rp - searchr:rp + searchr + 1]))\n",
    "    r_peaks3 = []\n",
    "    for rp in r_peaks2:\n",
    "        r_peaks3.append(rp - 2 + np.argmax(unfiltered_ecg[rp - 2:rp + 3])) # adjust by at most 2 samples to hit raw data max\n",
    "    return np.asarray(r_peaks3)\n",
    "\n",
    "\n",
    "def quotient_filter(hbpeaks, outlier_over=5, sampling_rate=128, tol=0.8):\n",
    "    '''\n",
    "    Function that applies a quotient filter similar to what is described in\n",
    "    \"Piskorki, J., Guzik, P. (2005), Filtering Poincare plots\"\n",
    "    it preserves peaks that are part of a sequence of [outlier_over] peaks with\n",
    "    a tolerance of [tol]'''\n",
    "    good_hbeats = []\n",
    "    good_rrs = []\n",
    "    good_rrs_x = []\n",
    "    for i, peak in enumerate(hbpeaks[:-(outlier_over-1)]):\n",
    "        hb_intervals = [hbpeaks[j]-hbpeaks[j-1]  for j in range(i+1, i+outlier_over)]\n",
    "        hr = 60/((sum(hb_intervals))/((outlier_over-1)*sampling_rate))\n",
    "        if min(hb_intervals) > max(hb_intervals)*tol and hr > 35 and hr < 185: # -> good data\n",
    "\n",
    "            for p in hbpeaks[i:i+outlier_over]:\n",
    "                if len(good_hbeats) == 0 or p > good_hbeats[-1]:\n",
    "                    good_hbeats.append(p)\n",
    "                    if len(good_hbeats) > 1:\n",
    "                        rr = good_hbeats[-1]-good_hbeats[-2]\n",
    "                        if rr<min(hb_intervals)/tol and rr>max(hb_intervals)*tol:\n",
    "                            good_rrs.append(rr)\n",
    "                            good_rrs_x.append((good_hbeats[-1]+good_hbeats[-2])/2)\n",
    "    return np.array(good_hbeats), np.array(good_rrs), np.array(good_rrs_x)\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = scipy.signal.butter(order, [low, high], analog=False, btype='band', output='sos')\n",
    "    return sos\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = scipy.signal.sosfiltfilt(sos, data)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee00167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(winsize: int = 8):\n",
    "    all_hrs = []\n",
    "    p = Path(\"C:/Users/cleme/ETH/Master/Thesis/data/WildPPG/data/WildPPG_Part_an0.mat\")\n",
    "\n",
    "    part_data = load_wildppg_participant(p.absolute())\n",
    "    import pdb \n",
    "    pdb.set_trace()\n",
    "    x = part_data[\"ankle\"][\"acc_x\"][\"v\"]\n",
    "    y = part_data[\"ankle\"][\"acc_y\"][\"v\"]\n",
    "    z = part_data[\"ankle\"][\"acc_z\"][\"v\"]\n",
    "    imu = np.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "    r_peaks = pan_tompkins_detector(part_data['sternum']['ecg']['v'], part_data['sternum']['ecg']['fs'])\n",
    "    ecgpks_filt, rrs, rrxs = quotient_filter(r_peaks, outlier_over=5, tol=0.75)\n",
    "\n",
    "    hrs = []\n",
    "    imus = []\n",
    "    for win_s in range(0, max(ecgpks_filt), winsize * part_data['sternum']['ecg']['fs']):\n",
    "        rr_in_win = rrs[np.logical_and(rrxs > win_s, rrxs < win_s + winsize * part_data['sternum']['ecg']['fs'])]\n",
    "        window_imu = imu[win_s: win_s + winsize * part_data['sternum']['ecg']['fs']]\n",
    "        imus.append(np.mean(window_imu))\n",
    "        if len(rr_in_win) > 1:  # at least 2\n",
    "            hrs.append(60 * len(rr_in_win) / (np.sum(rr_in_win) / part_data['sternum']['ecg']['fs']))\n",
    "        else:\n",
    "            hrs.append(0) # invalid / noisy ecg\n",
    "    all_hrs.append([[h] for h in hrs]) # ground truth hr in 8s windows, make col vec\n",
    "\n",
    "    outdict = {\"data_bpm_values\":np.asanyarray(all_hrs, dtype=object), \"imu\": np.array(imus)}\n",
    "    return outdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_supervised_windows(hr, imu, L=30, H=3, imu_lags=1):\n",
    "    \"\"\"\n",
    "    hr, imu: 1D numpy arrays (same length) for a single subject/sequence\n",
    "    L: lookback length used as HR_past\n",
    "    H: forecast horizon steps ahead (e.g., H=1,3,5,...)\n",
    "    imu_lags: number of causal IMU lags to include (1 => current t)\n",
    "\n",
    "    Returns:\n",
    "      Z: (N, L)   HR past (t-L+1 ... t)\n",
    "      X: (N, imu_lags)  IMU lags (t, t-1, ..., t-imu_lags+1)\n",
    "      y: (N,)  HR future at t+H\n",
    "    \"\"\"\n",
    "    assert hr.ndim == 1 and imu.ndim == 1 and len(hr) == len(imu)\n",
    "    T = len(hr)\n",
    "    max_lag = max(L, imu_lags - 1)\n",
    "    # last usable t is T-H-1 ; first usable t is max_lag\n",
    "    t_start = max_lag\n",
    "    t_end = T - H - 1\n",
    "    if t_end < t_start:\n",
    "        return np.empty((0, L)), np.empty((0, imu_lags)), np.empty((0,))\n",
    "\n",
    "    idx = np.arange(t_start, t_end + 1)\n",
    "    N = len(idx)\n",
    "\n",
    "    # HR past Z\n",
    "    Z = np.stack([hr[idx - k] for k in range(L, 0, -1)], axis=1)  # shape (N, L)\n",
    "\n",
    "    # IMU lags X (current to past)\n",
    "    X = np.stack([imu[idx - k] for k in range(imu_lags)], axis=1)  # (N, imu_lags)\n",
    "\n",
    "    # target y = future HR at t+H\n",
    "    y = hr[idx + H]\n",
    "    return Z, X, y\n",
    "\n",
    "\n",
    "def conditional_mutual_info(\n",
    "    hr, imu, L=30, H=3, imu_lags=1, n_neighbors=5, random_state=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimate I(IMU ; HR_future | HR_past) via difference of mutual infos:\n",
    "      I([IMU,Z]; y) - I(Z; y)\n",
    "    using sklearn's mutual_info_regression (kNN estimator).\n",
    "    Returns: cmi_estimate (nats), N_effective\n",
    "    \"\"\"\n",
    "    Z, X, y = make_supervised_windows(hr, imu, L=L, H=H, imu_lags=imu_lags)\n",
    "    if len(y) == 0:\n",
    "        return np.nan, 0\n",
    "\n",
    "    # Concatenate features\n",
    "    ZX = np.concatenate([Z, X], axis=1)\n",
    "\n",
    "    # sklearn returns MI in nats (natural log base)\n",
    "    I_ZY = mutual_info_regression(\n",
    "        Z, y, n_neighbors=n_neighbors, random_state=random_state\n",
    "    )\n",
    "    I_ZY_total = float(np.sum(I_ZY))  # sum over Z dims approximates I(Z; y)\n",
    "\n",
    "    I_ZX_Y = mutual_info_regression(\n",
    "        ZX, y, n_neighbors=n_neighbors, random_state=random_state\n",
    "    )\n",
    "    I_ZX_Y_total = float(np.sum(I_ZX_Y))  # approximates I([Z,X]; y)\n",
    "\n",
    "    cmi = I_ZX_Y_total - I_ZY_total\n",
    "    # CMI can't be < 0 theoretically; clip tiny negatives due to estimation noise\n",
    "    return max(0.0, cmi), len(y)\n",
    "\n",
    "def _standardize_train_apply(\n",
    "    X_train, X_val\n",
    "):\n",
    "    mu = X_train.mean(axis=0)\n",
    "    sd = X_train.std(axis=0) + 1e-12\n",
    "    return (X_train - mu) / sd, (X_val - mu) / sd, mu, sd\n",
    "\n",
    "def _make_lag_matrix(x, lags):\n",
    "    \"\"\"Return [x_{t-l} for l in lags] stacked column-wise, aligned to t.\"\"\"\n",
    "    cols = []\n",
    "    for l in range(1,lags+1):\n",
    "        if l == 0:\n",
    "            cols.append(x.copy())\n",
    "        else:\n",
    "            z = np.empty_like(x)\n",
    "            z[:l] = x[0]\n",
    "            z[l:] = x[:-l]\n",
    "            cols.append(z)\n",
    "    return np.column_stack(cols)\n",
    "\n",
    "def train_test_lr(hr, imu):\n",
    "\n",
    "    L = 30 \n",
    "    H = 3\n",
    "\n",
    "    train_end = int(0.7 * len(hr))\n",
    "\n",
    "    hr_train, hr_test, _, _  = _standardize_train_apply(hr[:train_end], hr[train_end:])\n",
    "    imu_train, imu_test, _, _ = _standardize_train_apply(imu[:train_end], imu[train_end:])\n",
    "\n",
    "    y_train = _make_lag_matrix(hr_train, lags=L)\n",
    "    x_train = _make_lag_matrix(imu_train, lags=L)\n",
    "\n",
    "    import pdb \n",
    "    pdb.set_trace() \n",
    "    \n",
    "    endo_lr = LinearRegression()\n",
    "    exo_lr = LinearRegression()\n",
    "\n",
    "    endo_lr.fit(y_train)\n",
    "    exo_lr.fit(np.concat((y_train, x_train),axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "04d66627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\cleme\\appdata\\local\\temp\\ipykernel_24324\\3866619099.py\u001b[0m(8)\u001b[0;36mprocess_data\u001b[1;34m()\u001b[0m\n",
      "\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'id', 'notes', 'sternum', 'head', 'ankle', 'wrist'])\n",
      "{'acc_x': {'fs': np.int32(128), 'descr': np.str_('x-axis of MEMS accelerometer'), 'v': array([0.92961694, 0.94211695, 0.93338305, ..., 0.936     , 0.9478826 ,\n",
      "       0.9516174 ])}, 'acc_y': {'fs': np.int32(128), 'descr': np.str_('y-axis of MEMS accelerometer'), 'v': array([ 0.04476611,  0.04423389,  0.0507661 , ..., -0.04      ,\n",
      "       -0.056     , -0.0563826 ])}, 'acc_z': {'fs': np.int32(128), 'descr': np.str_('z-axis of MEMS accelerometer'), 'v': array([-0.19676611, -0.17176611, -0.176     , ..., -0.152     ,\n",
      "       -0.1481174 , -0.12876521])}, 'altitude': {'fs': np.float64(0.5), 'descr': np.str_('barometric altitude measurements, averaged in 8-second moving windows, with step size 2 seconds'), 'v': array([466.03814316, 466.06812859, 466.10313249, ..., 483.3344698 ,\n",
      "       483.32486582, 483.3288753 ])}, 'temperature': {'fs': np.float64(0.5), 'descr': np.str_('temperature within device, averaged in 8-second moving windows, with step size 2 seconds'), 'v': array([28.78090332, 28.78798126, 28.79560623, ..., 31.22248917,\n",
      "       31.22292099, 31.22364243])}, 'ppg_g': {'fs': np.int32(128), 'descr': np.str_('reflective PPG at 530 nm wavelength'), 'v': array([0.20672607, 0.20634079, 0.20603943, ..., 0.27270317, 0.27392769,\n",
      "       0.27372551])}, 'ppg_ir': {'fs': np.int32(128), 'descr': np.str_('reflective PPG at 950 nm wavelength'), 'v': array([0.20640373, 0.20643425, 0.20649338, ..., 0.18877792, 0.1890049 ,\n",
      "       0.18916893])}, 'ppg_r': {'fs': np.int32(128), 'descr': np.str_('reflective PPG at 660 nm wavelength'), 'v': array([0.31987953, 0.31988144, 0.31988525, ..., 0.35540581, 0.35548782,\n",
      "       0.35558891])}}\n",
      "{'fs': np.int32(128), 'descr': np.str_('x-axis of MEMS accelerometer'), 'v': array([0.92961694, 0.94211695, 0.93338305, ..., 0.936     , 0.9478826 ,\n",
      "       0.9516174 ])}\n",
      "*** AttributeError: 'dict' object has no attribute 'shape'\n",
      "(5679104,)\n",
      "(5679104,)\n"
     ]
    }
   ],
   "source": [
    "sizes = [2,4,8]\n",
    "starts = [2070, 4300, 1200]\n",
    "ends = [2150, 4500, 1250]\n",
    "for winsize, start, end in zip(sizes, starts, ends):\n",
    "    outdict = process_data(winsize)\n",
    "    train_test_lr(outdict[\"data_bpm_values\"][0,:,0], outdict[\"imu\"])\n",
    "    print(conditional_mutual_info(outdict[\"data_bpm_values\"][0,:,0], outdict[\"imu\"]))\n",
    "    #plot_data(winsize, start, end, outdict)\n",
    "    #plot_scatter(outdict) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
