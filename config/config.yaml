defaults:
  - _self_
  - model: linear
  - dataset: wildppg
  - path: local
  - folds: fold_0
  - experiment: endo_only
  - lbw: a # hydra does not like integers! a:5, b:10, c:20, d:30, e:60
  # - pw: a # a: 3, b: 5, c: 10, d: 20, e: 30
  - optional params: ${model}/${dataset}/${lbw}
  # - optional override hydra/sweeper: ${model}_sweeper

seed: 123
overfit: False
num_workers: 0 # number of workers for pytorch dataloader 
use_plots: False # flag for plotting test predictions and test timeseries 

# data
use_heart_rate: True
n_folds: 3
fold_datasets: ["dalia", "wildppg", "ieee", "ucihar", "usc"] # these datasets have folds defined in config/experiments
local_datasets: ["fieee", "fdalia", "fwildppg"]
global_datasets: ["ieee", "dalia", "wildppg"]
normalization: "global" # can be either "global", "difference" or  "none"
train_frac: 0.7
val_frac: 0.1

numpy_models: ["linear","xgboost", "hlinear", "hxgboost", "exactgp"] # these models get passed the windowed training & val datasets 
special_models: ["linear","xgboost", "hlinear", "hxgboost", "hmm", "dynamax", "llmtime", "exactgp", "dummy"] # these models are not trained with Lightning 
probabilistic_models: ["gp", "dklgp", "exactgp"] # these models output the mean & std as the prediction

look_back_window: 5
prediction_window: 3

# Exogenous Variables Experiments
use_dynamic_features: ${experiment.use_dynamic_features}
use_static_features: ${experiment.use_static_features}
use_only_exo: ${experiment.use_only_exogenous_features}
use_perfect_info: ${experiment.use_perfect_info}
loss_fn: ${experiment.loss_fn} # can be "MSE", "SMAPE" or "MAE"

# Non-Stationary Experiments
use_norm_dl: ${experiment.use_norm_dl}  # indicates if we should use local z-normalization for DL models
use_norm_baseline: ${experiment.use_norm_baseline} # indicates if we should use local z-normalization for baselines

# wandb logging settings
use_wandb: False
wandb:
  save_dir: ${path.checkpoint_path}/wandb_logs
  entity: "c_keusch"
  project: "thesis"
  log_model: False
  save_code: False
  reinit: False

# tuner flag
tune: False
n_trials: 50

hydra:
  run:
    dir: ${path.basedir}/outputs



