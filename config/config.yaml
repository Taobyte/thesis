defaults:
  - _self_
  - base: base
  - path: local
  - model: linear
  - dataset: ieee
  - feature: none
  - folds: fold_0
  - lbw: a # hydra does not like integers for config groups! a:5, b:10, c:20, d:30, e:60
  - pw: a # a: 3, b: 5, c: 10, d: 20, e: 30
  - optional params: ${model}/${dataset}/${lbw}
  # - optional override hydra/sweeper: ${model}_sweeper

seed: 123
overfit: False # overfit on a single batch with lightning
num_workers: 0
min_delta: 0.001 # for early stopping callback
progress_bar: False # toggle progress bar when training

# data
n_folds: 3
local_datasets: ["lieee", "ldalia", "lwildppg", "lmitbih"]
global_datasets: ["ieee", "dalia", "wildppg"]
normalization: global # can be either "global", "minmax" or  "gnone"
local_norm: "local_z" # can be either "local_z", "difference" or "lnone" 
local_norm_endo_only: False # locally normalize only the hr series and leave exogenous variables untouched

look_back_window: 5
prediction_window: 3

# local settings
test_local: False # test on test windows from local for the global models
train_frac: 0.7
val_frac: 0.1

numpy_models: ["linear","xgboost","exactgp"] # these models get passed the windowed training & val datasets 
special_models: ["linear","xgboost", "exactgp", "dummy"] # these models are not trained with Lightning 
probabilistic_models: ["gp", "dklgp", "exactgp"] # these models output the mean & std as the prediction
return_series_models: ["msar"]

use_dynamic_features: ${feature.use_dynamic_features} 
use_static_features: False 
loss_fn: "MSE" # can be "MSE", "SMAPE" or "MAE"

# callbacks
use_efficiency_callback: False # compute efficiency related metrics
use_prediction_callback: False # store predictions + metrics in numpy .npz format

# tuner flag
tune: False
n_trials: 50

