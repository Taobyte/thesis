defaults:
  - _self_
  - model: linear
  - params: base 
  - dataset: dalia
  - path: local
  - folds: fold_0
  - experiment: endo_only
  - override hydra/sweeper: linear_sweeper

seed: 123
overfit: False
num_workers: 0 # how many workers the pytorch dataloader has
use_plots: False # flag for plotting test predictions and test timeseries 

# data
use_heart_rate: True
n_folds: 3
fold_datasets: ["dalia", "wildppg", "ieee", "ucihar", "usc"] # these datasets have folds defined in config/experiments
normalization: "global" # can be either "global", "difference" or  "none"

special_models: ["xgboost", "naive"] # these models are not trained with pytorch
probabilistic_models: ["gp", "dklgp", "exactgp", "bnn", "dropoutbnn"]

look_back_window: 5
prediction_window: 3

# Exogenous Variables Experiments
use_dynamic_features: ${experiment.use_dynamic_features}
use_static_features: ${experiment.use_static_features}
use_only_exo: ${experiment.use_only_exogenous_features}
use_perfect_info: ${experiment.use_perfect_info}
loss_fn: ${experiment.loss_fn} # can be "MSE", "SMAPE" or "MAE"

# Non-Stationary Experiments
use_norm_dl: ${experiment.use_norm_dl}  # indicates if we should use local z-normalization for DL models
use_norm_baseline: ${experiment.use_norm_baseline} # indicates if we should use local z-normalization for baselines
# RevIN

# wandb logging settings
use_wandb: False
wandb:
  save_dir: ${path.checkpoint_path}/wandb_logs
  entity: "c_keusch"
  project: "thesis"
  log_model: False
  save_code: False
  reinit: False

# tuner flag
tune: False
n_trials: 50
  
# multi gpu settings
use_multi_gpu: False
multi:
  accelerator: "gpu"
  devices: 4
  num_nodes: 1
  strategy: "ddp"

hydra:
  run:
    dir: ${path.basedir}/outputs



