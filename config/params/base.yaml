# baselines
# we can add in specific parameters with:  ${params.${dataset.name}.${str:${use_dynamic_features}}.${str:${look_back_window}}.${str:${prediction_window}}.learning_rate} 
linear:
  learning_rate: 0.001
  weight_decay: 6.627291754394113e-06

hmm:
  learning_rate: 0.001
  hidden_dim: 32

kalmanfilter:
  learning_rate: 0.001
  hidden_dim: 32

xgboost:
  n_estimators: 600
  learning_rate: 0.01
  max_depth: 5
  reg_alpha: 0.0001
  reg_lambda: 0.0
  subsample: 0.7
  colsample_bytree: 0.8

gp:
  n_points: 250
  learning_rate: 0.001
  num_latents: 10
  kernel: "matern"
  use_linear_trend: 1
  periodic_type: ""


# SOTA Deep Learning Architectures

timesnet:
  learning_rate: 0.001
  e_layers: 2
  num_kernels: 6
  top_k: 5
  d_model: 32
  d_ff: 32
  dropout: 0.01

simpletm:
  learning_rate: 0.004
  e_layers: 1
  d_model: 256
  d_ff: 32
  dropout: 0.015
  geomattn_dropout: 0.25

adamshyper:
  learning_rate: 0.0023752166039571958
  hyper_num: [50, 20, 10]
  d_model: 768
  inner_size: 16

timexer:
  d_ff: 256 # 128, 256, 512
  d_model: 128 # 128, 256, 512 
  e_layers: 4 # 2,3 
  patch_len: 2
  use_norm: True
  dropout: 0.1

gpt4ts:
  learning_rate: 0.0001
  patch_size: 1
  stride: 1
  d_ff: 128
  d_model: 64
  gpt_layers: 6




pattn:
  learning_rate: 0.002428582457141732
  tmax: 10
  patch_size: 4 
  d_model: 128
