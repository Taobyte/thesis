name: "xgboost"

model:
  _target_: src.models.xgboost.XGBoostModel
  learning_rate: 0.05

  objective: "reg:squarederror"
  n_estimators: 300

  max_depth: 4
  reg_alpha: 1
  reg_lambda: 1 
  subsample: 0.8
  colsample_bytree: 0.8

pl_model:
  _target_: src.models.xgboost.XGBoost
  loss: "MSE"
  base_channel_dim: ${dataset.base_channel_dim}

trainer:
  max_epochs: 0 # we don't train here, because we use the flattened dataset at once and don't use batched training
  use_early_stopping: False

data:
  batch_size: 32