name: "hxgboost"

model:
  _target_: src.models.hxgboost.HXGBoostModel
  loss: ${loss_fn}
  learning_rate: 0.05               
  n_estimators: 500                 
  max_depth: 6                      
  reg_alpha: 0.1                    
  reg_lambda: 1.0                   
  subsample: 0.8                    
  colsample_bytree: 0.8            
  use_early_stopping: ${model.trainer.use_early_stopping}
  patience: ${model.trainer.patience}

pl_model:
  _target_: src.models.hxgboost.HXGBoost
  loss: ${loss_fn}
  use_norm: ${use_norm_baseline}
  verbose: True

trainer:
  max_epochs: 1 # we don't train here, because we use the flattened dataset at once and don't use batched training
  use_early_stopping: True
  patience: 2

data:
  batch_size: 256