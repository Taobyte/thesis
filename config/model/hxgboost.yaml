name: "hxgboost"

model:
  _target_: src.models.hxgboost.HXGBoostModel
  loss: ${loss_fn}
  learning_rate: ${params.xgboost.learning_rate} 
  n_estimators: ${params.xgboost.n_estimators}
  max_depth: ${params.xgboost.max_depth} 
  reg_alpha: ${params.xgboost.reg_alpha}
  reg_lambda: ${params.xgboost.reg_lambda}
  subsample: ${params.xgboost.subsample}
  colsample_bytree: ${params.xgboost.colsample_bytree}
  use_early_stopping: ${model.trainer.use_early_stopping}
  patience: ${model.trainer.patience}

pl_model:
  _target_: src.models.hxgboost.HXGBoost
  loss: ${loss_fn}
  use_norm: ${use_norm_baseline}
  verbose: True

trainer:
  max_epochs: 1 # we don't train here, because we use the flattened dataset at once and don't use batched training
  use_early_stopping: True
  patience: 2

data:
  batch_size: 256