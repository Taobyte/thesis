name: "xgboost"

model:
  _target_: src.models.xgboost.XGBoostModel
  # objective: "reg:squarederror"
  loss: ${loss_fn}
  learning_rate: 0.01
  n_estimators: 600 
  max_depth: 5 
  reg_alpha: 0.0 
  reg_lambda: 0.0 
  subsample: 0.7 
  colsample_bytree: 0.8 
  use_early_stopping: ${model.trainer.use_early_stopping}
  patience: ${model.trainer.patience}

pl_model:
  _target_: src.models.xgboost.XGBoost
  loss: ${loss_fn}
  use_norm: ${use_norm_baseline}
  verbose: False

trainer:
  max_epochs: 1 # we don't train here, because we use the flattened dataset at once and don't use batched training
  use_early_stopping: True
  patience: 2

data:
  batch_size: 256