name: "xgboost"

model:
  _target_: src.models.xgboost.XGBoostModel
  objective: "reg:squarederror"
  learning_rate: 1.0 # 0.006271509506565111
  n_estimators: 50 # 638
  max_depth: 3 # 5 
  reg_alpha: 5.0 # 0.0001442801684589295
  reg_lambda: 5.0 # 1.1942150027444449e-05
  subsample: 0.8 # 0.6921657132206152
  colsample_bytree: 0.8473304514929147
  use_early_stopping: ${model.trainer.use_early_stopping}
  patience: ${model.trainer.patience}

pl_model:
  _target_: src.models.xgboost.XGBoost
  loss: "MSE"
  target_channel_dim: ${dataset.datamodule.target_channel_dim}

trainer:
  max_epochs: 1 # we don't train here, because we use the flattened dataset at once and don't use batched training
  use_early_stopping: False
  patience: 2

data:
  batch_size: 256