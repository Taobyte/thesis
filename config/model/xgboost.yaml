name: "xgboost"

model:
  _target_: src.models.xgboost.XGBoostModel
  objective: "reg:squarederror"
  learning_rate: 0.05
  n_estimators: 300
  max_depth: 4
  reg_alpha: 1
  reg_lambda: 1 
  subsample: 0.8
  colsample_bytree: 0.8

pl_model:
  _target_: src.models.xgboost.XGBoost
  loss: "MSE"
  target_channel_dim: ${dataset.datamodule.target_channel_dim}

trainer:
  max_epochs: 1 # we don't train here, because we use the flattened dataset at once and don't use batched training
  use_early_stopping: False

data:
  batch_size: 256