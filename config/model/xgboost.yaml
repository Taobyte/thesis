name: "xgboost"

model:
  _target_: src.models.xgboost.XGBoostModel
  objective: "reg:squarederror"
  learning_rate: ${params.xgboost.learning_rate} 
  n_estimators: ${params.xgboost.n_estimators}
  max_depth: ${params.xgboost.max_depth} 
  reg_alpha: ${params.xgboost.reg_alpha}
  reg_lambda: ${params.xgboost.reg_lambda}
  subsample: ${params.xgboost.subsample}
  colsample_bytree: ${params.xgboost.colsample_bytree}
  use_early_stopping: ${model.trainer.use_early_stopping}
  patience: ${model.trainer.patience}

pl_model:
  _target_: src.models.xgboost.XGBoost
  loss: "MSE"
  target_channel_dim: ${dataset.datamodule.target_channel_dim}
  verbose: True

trainer:
  max_epochs: 1 # we don't train here, because we use the flattened dataset at once and don't use batched training
  use_early_stopping: True
  patience: 2

data:
  batch_size: 256