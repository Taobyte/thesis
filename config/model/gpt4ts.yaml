name: "gpt4ts"

model:
  _target_: src.models.gpt4ts.Model
  seq_len: ${look_back_window}
  pred_len: ${prediction_window}
  ln: 0 
  enc_in: ${dataset.datamodule.look_back_channel_dim}
  c_out: ${dataset.datamodule.look_back_channel_dim}
  
  # tunable parameters
  patch_size: ${params.gpt4ts.patch_size} # default 16
  stride: ${params.gpt4ts.stride}
  d_ff: ${params.gpt4ts.d_ff}
  d_model: ${params.gpt4ts.d_model} # max dimension is 768
  gpt_layers: ${params.gpt4ts.gpt_layers}
  
  embed: 'timeF' # time features encoding, options:[timeF, fixed, learned]
  freq: 'm'

  use_norm: ${use_norm_dl}


pl_model: 
  _target_: src.models.gpt4ts.GPT4TS
  learning_rate: ${params.gpt4ts.learning_rate}
  loss: ${loss_fn}
  lradj: "type1" # lr * 0.5**(epoch - 1 // 1)
  # lradj: "none"


trainer: 
  max_epochs: 50
  use_early_stopping: False
  patience: 5

data:
  batch_size: 64