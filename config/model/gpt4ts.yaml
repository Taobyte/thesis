name: "gpt4ts"

model:
  _target_: src.models.gpt4ts.Model
  seq_len: ${look_back_window}
  pred_len: ${prediction_window}
  ln: 0 
  enc_in: ${dataset.datamodule.look_back_channel_dim}
  c_out: ${dataset.datamodule.look_back_channel_dim}
  patch_size: 1 
  stride: 1 
  embed: 'timeF' # time features encoding, options:[timeF, fixed, learned]
  freq: 'm'
  
  # tunable parameters
  d_ff: 128
  d_model: 128 # max dimension is 768
  gpt_layers: 6 
  dropout: 0.1 
  
  use_norm: ${use_norm_dl}

pl_model: 
  _target_: src.models.gpt4ts.GPT4TS
  learning_rate: 0.0001
  loss: ${loss_fn}
  lradj: "type1" # lr * 0.5**(epoch - 1 // 1)
  # lradj: "none"

trainer: 
  max_epochs: 1000
  use_early_stopping: False
  patience: 5

data:
  batch_size: 32