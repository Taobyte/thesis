defaults:
  - base_module

name: "nbeatsx"

model:
  _target_: src.models.nbeatsx.Model
  input_size: ${look_back_window}
  output_size: ${prediction_window}
  exogenous_n_channels: 1 
  shared_weights: False 
  activation: "prelu" # the authors observe better performance with prelu, selu or sigmoid than with relu
  initialization: "glorot_uniform"
  
  batch_normalization: False  # the authors use no batch normalization
  x_s_n_hidden: 0 
  n_x_s: 0 # we do not have static exogenous variables

  # tunable parameters
  stack_types: ["exogenous_wavenet", "identity"]
  n_blocks: [1, 1] # length must be equal to len(stack_types)
  n_layers: [2, 2] # length must be equal to len(stack_types) | number of FCNN layers in each block and stack
  n_hidden: 50 # the number of hidden units in a FCN layer in each block in each stack!
  kernel_size: 5 

  n_polynomials: 2 # the authors observe best performance with 2 polynomials
  dropout_prob_theta: 0.0
  dropout_prob_exogenous: 0.0

  
pl_model: 
  _target_: src.models.nbeatsx.NBeatsX
  learning_rate: 0.001 
  weight_decay: 0.0 # is in (1e-5, 1.0)
  n_lr_decay_steps: 3 # the authors half the learning rate at most 3 times per training run
  loss_fn: "MAE" # the authors use the L1 loss instead of MSE
  use_norm: ${use_norm_dl}

trainer: 
  max_epochs: 100
  use_early_stopping: True
  patience: 5

data:
  batch_size: 256