model_name: "elastst"
model:
  _target_: src.models.elastst.Model
  l_patch_size: '8_16_32'
  dropout: 0.0
  f_hidden_size: 256
  d_inner: 256
  t_layers: 2
  v_layers: 0
  n_heads: 8
  d_v: 64
  d_k: 64
  structured_mask: true
  rotate: true
  rope_theta_init: 'exp'
  learnable_rope: true
  min_period: 1
  max_period: 1000
  addv: false
  bin_att: false
  learn_tem_emb: false

  target_dim: ${dataset.base_channel_dim}
  context_length: ${look_back_window}
  prediction_length: ${prediction_window}
  freq: ${dataset.freq}

pl_model:
  _target_: src.models.elastst.ElasTST
  learning_rate: 0.001
  quantiles_num: 20
  sampling_weight_scheme: random

trainer:
  use_early_stopping: False
  accelerator: cpu
  devices: 1
  strategy: auto
  max_epochs: 50
  use_distributed_sampler: false
  limit_train_batches: 100
  log_every_n_steps: 1
  default_root_dir: ./results
  accumulate_grad_batches: 1

data:
  data_manager:
    _target_: src.datasets.elastst.data_manager.DataManager
    dataset_cfg: ${dataset}
    scaler: standard # identity, standard, temporal
    context_length: 50
    prediction_length: [1, 2, 5, 8, 9, 10]
    train_ctx_len: 50
    train_pred_len_list: 10
    val_ctx_len: 50
    val_pred_len_list: 10
    continuous_sample: false
  batch_size: 32